{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe21672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join('..'))\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "sys.path.append(os.path.join('..', 'src', 'libs'))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "from mfethuls import parse as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790d125e",
   "metadata": {},
   "source": [
    "### Test current parser implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8314fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw = 'uv'\n",
    "df = pa.get_data(pa.path_constructor(kw, 'LUB038'), kw)\n",
    "# data = pa.get_data(pa.path_constructor('uv', *exps_reps), 'uv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85d6b5c",
   "metadata": {},
   "source": [
    "### General funtions - parsers interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d5afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_constructor(instrmnt_kw, *args):\n",
    "\n",
    "    # Load .env into enviroment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Path to folder containing instrument data\n",
    "    path = os.environ.get('PATH_TO_DATA')\n",
    "    env_suffix = f'{instrmnt_kw.upper()}_FOLDER_NAME'\n",
    "    path = os.path.join(path, os.environ.get(env_suffix))\n",
    "    \n",
    "    \n",
    "    # Folders/Files insterested in for analysis\n",
    "    if [*args]:\n",
    "        args = [*args]\n",
    "    \n",
    "    # Create dictionary of folders in accordance with args and folders present\n",
    "    dict_paths = {}\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        name = [os.path.normpath(root).split(os.path.sep)[-1] for name in args if name in root]\n",
    "        if name:\n",
    "            dict_paths[name[0]] = [os.path.join(root, f) for f in sorted(files)]\n",
    "    \n",
    "    if not [*sum([*dict_paths.values()], [])] and not os.path.exists(path):\n",
    "        raise KeyError(f'path: {path} does not exist')\n",
    "    \n",
    "    return dict_paths\n",
    "\n",
    "\n",
    "# Construct dataframe from different instruments via walk through paths\n",
    "def get_data(dict_paths, instrmnt_kw):\n",
    "    instrmnt_kw_lwr = instrmnt_kw.lower()\n",
    "\n",
    "    dict_df = {}\n",
    "    for name, paths in dict_paths.items():\n",
    "        df = pd.DataFrame()\n",
    "        for path in paths:\n",
    "            print(path)\n",
    "\n",
    "            # TODO: develop method to read type of file - user decides on path\n",
    "            if instrmnt_kw_lwr == 'uv':\n",
    "                df = parse_uvvis(df, path)\n",
    "\n",
    "            elif instrmnt_kw_lwr == 'ftir':\n",
    "                df = parse_ftir(df, path)\n",
    "\n",
    "            elif instrmnt_kw_lwr == 'tga':\n",
    "                df = parse_tga(df, path)\n",
    "\n",
    "            elif instrmnt_kw_lwr == 'dsc':\n",
    "                df = parse_dsc(df, path)\n",
    "\n",
    "            else:\n",
    "                raise KeyError(f'The instrument keyword {instrmnt_kw} is not found')\n",
    "    \n",
    "        if not df.empty:\n",
    "            dict_df[name] = df\n",
    "\n",
    "    return dict_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93df42e",
   "metadata": {},
   "source": [
    "### UV-Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c53be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in .txt files from UV-Vis Shimadzu\n",
    "def get_uvvis_df(path): return pd.read_csv(path, skiprows=lambda x: x in [0, 2], sep='\\t') \\\n",
    "    .set_index('Wavelength nm.') \\\n",
    "    .rename(columns={'Abs.': os.path.basename(os.path.normpath(path)).split('_')[-1].rstrip('.txt').lstrip('0')}) \\\n",
    "    .rename(columns={'': '0'}) \\\n",
    "    .astype(float)\n",
    "\n",
    "# Concat each read and indicate files not read\n",
    "def parse_uvvis(df: pd.DataFrame, path: str) -> pd.DataFrame:\n",
    "    if '.txt' in path[-4:]:\n",
    "\n",
    "        df = pd.concat([df, get_uvvis_df(path)], axis=1).dropna(how='all', axis=1)\n",
    "\n",
    "    else:\n",
    "        print(f'Not reading: {path}')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0c3606",
   "metadata": {},
   "source": [
    "### FTIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1975e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs to be improved on.\n",
    "def get_ftir_df(path): return pd.read_csv(path, skiprows=lambda x: x in [0, 0], sep=',') \\\n",
    "    .set_index('cm-1') \\\n",
    "    .rename(columns={'%T': os.path.basename(os.path.normpath(path)).split('_')[-1].rstrip('.csv').lstrip('0')}) \\\n",
    "    .astype(float)\n",
    "\n",
    "def parse_ftir(df: pd.DataFrame, path: str) -> pd.DataFrame:\n",
    "    if '.csv' in path[-4:]:\n",
    "\n",
    "        df = pd.concat([df, get_ftir_df(path)], axis=1).dropna(how='all', axis=1)\n",
    "\n",
    "    else:\n",
    "        print(f'Not reading: {path}')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72db8da",
   "metadata": {},
   "source": [
    "### TGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5eda5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tga_df(path):\n",
    "\n",
    "    lines = []\n",
    "    with open(path) as f:\n",
    "        take = 0\n",
    "        for line in f.readlines():\n",
    "\n",
    "            if take==1:\n",
    "                l = re.split('\\s+', line.strip(), maxsplit=5)\n",
    "                lines.append(l)\n",
    "\n",
    "            if 'Index' in line:\n",
    "                cols = re.split('\\s+', line.strip(), maxsplit=5)\n",
    "                take=1\n",
    "            elif 'Results' in line:\n",
    "                take=0\n",
    "\n",
    "    return pd.DataFrame(lines, columns=cols).apply(pd.to_numeric, errors='coerce').dropna() \\\n",
    "             .rename(columns={'Value': f'Value_{os.path.basename(os.path.normpath(path)).rstrip(\".txt\")}'}) \\\n",
    "             .set_index('Tr') \\\n",
    "             .drop(columns=['Index', 't', 'Ts'])\n",
    "\n",
    "def parse_tga(df: pd.DataFrame, path: str) -> pd.DataFrame:\n",
    "    if '.txt' in path[-4:]:\n",
    "\n",
    "        df = pd.concat([df, get_tga_df(path)], axis=1).dropna(how='all', axis=1)\n",
    "\n",
    "    else:\n",
    "        print(f'Not reading: {path}')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eaeefa",
   "metadata": {},
   "source": [
    "### DSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262cdec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dsc_df(path):\n",
    "\n",
    "    lines = []\n",
    "    with open(path) as f:\n",
    "        take = 0\n",
    "        for line in f.readlines():\n",
    "\n",
    "            if take==1:\n",
    "                l = re.split('\\s+', line.strip(), maxsplit=5)\n",
    "                lines.append(l)\n",
    "\n",
    "            if 'Index' in line:\n",
    "                cols = re.split('\\s+', line.strip(), maxsplit=5)\n",
    "                take=1\n",
    "            elif 'Results' in line:\n",
    "                take=0\n",
    "\n",
    "    df = pd.DataFrame(lines, columns=cols).apply(pd.to_numeric, errors='coerce').dropna() \\\n",
    "           .drop(columns=['Index', 't', 'Ts'])\n",
    "    df['name'] = [f'{os.path.basename(os.path.normpath(path)).rstrip(\".txt\")}'] * len(df.Tr)\n",
    "    \n",
    "    \n",
    "    # TODO: Make more elegant >:\n",
    "    # Cut heating, cooling and isothermal cycles - label accordingly\n",
    "    df['cycle'] = ['Isothermal'] * len(df.Tr)\n",
    "    df['differ'] = df.Tr.diff()\n",
    "    df['differ_1'] = df.differ.diff()\n",
    "\n",
    "    df.loc[df.differ > 0, 'cycle'] = 'Heating'\n",
    "    df.loc[df.differ < 0, 'cycle'] = 'Cooling'\n",
    "    df.loc[(df.differ_1 < -0.1) & (df.cycle != 'Isothermal'), 'cycle'] = 'Cooling_start'\n",
    "    df.loc[(df.differ_1 < -0.1) & (df.cycle == 'Isothermal'), 'cycle'] = 'Heating_end'\n",
    "    df.loc[(df.differ_1 > 0.1) & (df.cycle != 'Isothermal'), 'cycle'] = 'Heating_start'\n",
    "    df.loc[(df.differ_1 > 0.1) & (df.cycle == 'Isothermal'), 'cycle'] = 'Cooling_end'\n",
    "\n",
    "\n",
    "    heating_cycle_num = 0\n",
    "    cooling_cycle_num = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if row.differ > 0.0:\n",
    "            if 'Heating_end' not in row.cycle:\n",
    "                df.loc[index, 'cycle'] = df.loc[index, 'cycle'] + f'_{str(heating_cycle_num)}'\n",
    "            else:\n",
    "                heating_cycle_num += 1\n",
    "\n",
    "        elif row.differ < 0.0:\n",
    "            if 'Cooling_end' not in row.cycle:\n",
    "                df.loc[index, 'cycle'] = df.loc[index, 'cycle'] + f'_{str(cooling_cycle_num)}'\n",
    "            else:\n",
    "                cooling_cycle_num += 1\n",
    "\n",
    "        else:\n",
    "            if 'Heating_end' in row.cycle:\n",
    "                heating_cycle_num += 1\n",
    "            elif 'Cooling_end' in row.cycle:\n",
    "                cooling_cycle_num += 1\n",
    "    \n",
    "    return df.drop(columns=['differ', 'differ_1'])\n",
    "\n",
    "def parse_dsc(df: pd.DataFrame, path: str) -> pd.DataFrame:\n",
    "    if '.txt' in path[-4:]:\n",
    "\n",
    "        df = pd.concat([df, get_dsc_df(path)], axis=0).dropna(how='all', axis=1)\n",
    "\n",
    "    else:\n",
    "        print(f'Not reading: {path}')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaa74bc",
   "metadata": {},
   "source": [
    "### RHEOMETER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ccee61-6202-45e2-8b34-a9dc113a209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pa.get_data(pa.path_constructor('Rheology', 'LUB080'), 'rheo')\n",
    "# pa.path_constructor('Rheology', 'LUB080')\n",
    "dev_path = 'C:\\\\Users\\\\BertossL\\\\Documents\\\\Rheology\\\\LUB080\\\\lub080_uv0.csv'\n",
    "\n",
    "# with open(dev_path, 'r') as f:\n",
    "#     contents = f.read()\n",
    "\n",
    "df = pd.read_csv('C:\\\\Users\\\\BertossL\\\\Documents\\\\Rheology\\\\LUB080\\\\lub080_uv0.txt', engine='python',\n",
    "            encoding='utf-8', on_bad_lines='skip', skip_blank_lines=True, header=[4, 6], sep='\\t') \\\n",
    "            .dropna(how='all') \\\n",
    "            .reset_index(drop=True) \\\n",
    "            .sort_index(axis=1) \\\n",
    "            .drop(columns=['Interval data:', 'Point No.'])\n",
    "df.columns = df.columns.get_level_values(0) + [f' {col}' if 'Unnamed' not in col else f'' for col in df.columns.get_level_values(1)]\n",
    "df.loc[:, 'name'] = os.path.basename(os.path.normpath(dev_path)).split('$')[0]\n",
    "df.loc[:, 'test_type'] = os.path.basename(os.path.normpath(dev_path)).split('$')[-1].rstrip('.csv').strip('0')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f09463-ba3c-4454-9db4-49e0610d4878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024236c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can try and improve on.\n",
    "def get_rheo_df(path): \n",
    "    \n",
    "    # Quite a shitty parse\n",
    "    df = pd.read_csv(path, engine='python', encoding='utf-8', on_bad_lines='skip', skip_blank_lines=True, header=[4, 6], sep='\\t') \\\n",
    "           .dropna(how='all') \\\n",
    "           .reset_index(drop=True) \\\n",
    "           .sort_index(axis=1) \\\n",
    "           .drop(columns=['Interval data:', 'Point No.']) \n",
    "    \n",
    "    # Rename columns\n",
    "    df.columns = df.columns.get_level_values(0) + [f' {col}' if 'Unnamed' not in col else f'' for col in df.columns.get_level_values(1)]\n",
    "    df.loc[:, 'name'] = os.path.basename(os.path.normpath(dev_path)).split('_')[0].rstrip('.csv')\n",
    "    df.loc[:, 'test_type'] = os.path.basename(os.path.normpath(dev_path)).split('_')[-1].rstrip('.csv').strip('0')\n",
    "\n",
    "    return df\n",
    "\n",
    "def parse_rheo(df: pd.DataFrame, path: str) -> pd.DataFrame:\n",
    "    if '.csv' in path[-4:]:\n",
    "\n",
    "        df = pd.concat([df, get_ftir_df(path)], axis=1).dropna(how='all', axis=0)\n",
    "\n",
    "    else:\n",
    "        print(f'Not reading: {path}')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b3235",
   "metadata": {},
   "source": [
    "### Test parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d5fd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw = 'dsc'\n",
    "df = get_data(path_constructor(kw), kw).get('DSC')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577ca620",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.name.unique())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "heatings_bkb = df.loc[(df.name == '!$BKBZn_DSC') & (df.cycle.str.contains('Heating')), :]\n",
    "heatings_bkbmbtt = df.loc[(df.name == '!$BKBMBTTZn_DSC') & (df.cycle.str.contains('Heating')), :]\n",
    "heatings_bkbmbttex = df.loc[(df.name == '!$LUB026_BKBZn_MBTT_UVex_DSC') & (df.cycle.str.contains('Heating')), :]\n",
    "heatings_bkbex = df.loc[(df.name == '!$LUB_BKBZn_noMBTT_UVex_DSC') & (df.cycle.str.contains('Heating')), :]\n",
    "\n",
    "\n",
    "# plt.plot(heatings.Tr, heatings.Value, '.',)\n",
    "# plt.plot(heatings[(heatings.cycle.str.contains('1'))].Tr, heatings[(heatings.cycle.str.contains('1'))].Value, 'r.')\n",
    "\n",
    "# plt.plot(heatings_bkbmbtt.Tr, heatings_bkbmbtt.Value, '.', color='steelblue')\n",
    "plt.plot(heatings_bkbmbtt[(heatings_bkbmbtt.cycle.str.contains('1'))].Tr, heatings_bkbmbtt[(heatings_bkbmbtt.cycle.str.contains('1'))].Value, '.', color='deepskyblue')\n",
    "\n",
    "# plt.plot(heatings_bkbmbttex.Tr, heatings_bkbmbttex.Value, '.', color='forestgreen')\n",
    "plt.plot(heatings_bkbmbttex[(heatings_bkbmbttex.cycle.str.contains('1'))].Tr, heatings_bkbmbttex[(heatings_bkbmbttex.cycle.str.contains('1'))].Value, '.', color='limegreen')\n",
    "\n",
    "# plt.plot(heatings_bkbex.Tr, heatings_bkbex.Value, 'm.')\n",
    "# plt.plot(heatings_bkbex[(heatings_bkbex.cycle.str.contains('1'))].Tr, heatings_bkbex[(heatings_bkbex.cycle.str.contains('1'))].Value, 'g.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04be2e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coolings = df.loc[(df.name == '!$BKBMBTTZn_DSC') & (df.cycle.str.contains('Cooling')), :]\n",
    "coolings_1 = df.loc[(df.name == '!$LUB026_BKBZn_MBTT_UVex_DSC') & (df.cycle.str.contains('Cooling')), :]\n",
    "\n",
    "plt.plot(coolings.Tr, coolings.Value, '.', color='deepskyblue')\n",
    "plt.plot(coolings_1.Tr, coolings_1.Value, '.', color='limegreen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3581565",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name\n",
    "df.loc[(df.name == '!$BKB1_DSC') & (df.cycle.str.contains('Heating')), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5abd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw = 'tga'\n",
    "path_constructor(kw)\n",
    "dict_df = get_data(path_constructor(kw), kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d26dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tga = dict_df.get('TGA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84394f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tga.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e957fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tga.loc[:, ['Value_LUB026_BKBZN_noMBTT_UVex_TGA', 'Value_LUB_BKB1']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1d55bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tga.loc[:, ['Value_LUB026_BKBZN_MBTT_UVex_TGA', 'Value_LUB_BKBMBTT']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d117cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw = 'Rheology'\n",
    "df = pa.get_data(pa.path_constructor(kw, 'LUB081'), kw)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c6aae8-2d02-4393-8281-4a06f0b8b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(df.test_type == 'freqSweep').dropna(how='all', axis=0).dropna(how='all', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb94691-c4bb-4897-afdd-fd6620258603",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d339cccf-2977-4e72-99d6-820fb843df99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
